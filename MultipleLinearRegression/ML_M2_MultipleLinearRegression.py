# -*- coding: utf-8 -*-
"""ML-2-MultipleLinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EZQTodMmsYb308buM0PZjk0nU3Poim4J
"""

#importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



#importing dataset

dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values
Y = dataset.iloc[:, -1].values
#print(X)



#here we have state as categorical data so we will performe onehot encodng to convert it to binary vector

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
X = np.array(ct.fit_transform(X)) 
#print(X)



#spliting dataset into trainset and testset

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)
#print(X_train)
#print(X_test)
#print(Y_train)
#print(Y_test)


#training our multiple LR model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, Y_train)


#predicting the results
Y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print("real value" + " predicted values")
print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#optional code-building backward elimination
#import statsmodels.api as sm

#here we will first add b0(constant)
#X = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)

#step1:intialize x_opt with all the independent variables
#X_opt = X[:, [0, 1, 2, 3, 4, 5]]
#X_opt = X_opt.astype(np.float64)

#fit the all model with all possible prediction
#making obj of OLS class=ordinary least sqaure
#regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()

#here summary function return the feture with highest P-value and other stuff:we will exclude the feture which has lower P-value
#regressor_OLS.summary()

#repeat1-removed 2(p value:0.990)
#X_opt = X[:, [0, 1, 3, 4, 5]]
#X_opt = X_opt.astype(np.float64)
#regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
#regressor_OLS.summary()

#repeat2:remove 1(p value:0.950)
#X_opt = X[:, [0, 3, 4, 5]]
#X_opt = X_opt.astype(np.float64)
#regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
#regressor_OLS.summary()

#remove4:
#X_opt = X[:, [0, 3, 5]]
#X_opt = X_opt.astype(np.float64)
#regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
#regressor_OLS.summary()

#final:feture with p value nearly 0
#X_opt = X[:, [0, 3]]
#X_opt = X_opt.astype(np.float64)regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
#regressor_OLS.summary()
